{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA using testing using smaller data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('sentiment/movies.csv')\n",
    "movies_doc = movies_df.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric_or_space = re.compile('[^(\\w|\\s|\\d)]')\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces,\n",
    "                 strip_numeric, remove_stopwords, strip_short,stem_text]\n",
    "\n",
    "def preprocess(doc): \n",
    "    words = preprocess_string(doc, filters=CUSTOM_FILTERS)\n",
    "    doc = ' '.join(words).lower()\n",
    "    return re.sub(not_alphanumeric_or_space, '', doc)\n",
    "\n",
    "def docs_prep(docs):\n",
    "    processed_docs = []\n",
    "    for i in docs:\n",
    "        movies_vec = preprocess(i)\n",
    "        processed_docs.append(movies_vec)  \n",
    "    new_prep = []\n",
    "    for i in processed_docs:\n",
    "        to_add = i.split()\n",
    "        new_prep.append(to_add)\n",
    "    return new_prep\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2csc, Sparse2Corpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import re\n",
    "from gensim.parsing.preprocessing import preprocess_string, split_alphanum, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric,remove_stopwords ,strip_short ,stem_text\n",
    "from pprint import pprint\n",
    "\n",
    "def sql_to_list_converter(chunks):\n",
    "    preprocessed_texts = []\n",
    "    for i in chunks:\n",
    "        \n",
    "        #have to split each sentence since this is the input for gensim\n",
    "        list_to_add = [iterator.split() for iterator in i.preprocessed_text.tolist()]\n",
    "        preprocessed_texts += list_to_add\n",
    "        \n",
    "    return preprocessed_texts\n",
    "\n",
    "def tf_idf_vect(preprocessed_texts):\n",
    "    dictionary = Dictionary(preprocessed_texts)\n",
    "    \n",
    "    # remove words that appear in less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "    num_docs = dictionary.num_docs\n",
    "    num_terms = len(dictionary.keys())\n",
    "    \n",
    "    #transform into bow\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in preprocessed_texts]\n",
    "    \n",
    "    #transform into tf-idf:\n",
    "    tfidf = TfidfModel(corpus_bow) #,normalize = True)\n",
    "    corpus_tfidf = tfidf[corpus_bow]\n",
    "    \n",
    "    #transform into sparse matrix:\n",
    "    corpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms,num_docs=num_docs)\n",
    "    \n",
    "                       \n",
    "    #NB: After reading online, LDA works just as well and even better in some case\n",
    "    #with corpus_tfidf, so I can use the tfidf instead\n",
    "    #NB: need to return dictionary to use in the LDA model\n",
    "    \n",
    "    return dictionary, corpus_tfidf, corpus_bow #, corpus_tfidf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doccc = docs_prep(movies_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 196 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dic, corpus_tfidf, corpus_bow = tf_idf_vect(doccc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using corpus tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.06438062249818663),\n",
       " (1, 0.05331512447771581),\n",
       " (2, 0.129758482602989),\n",
       " (3, 0.0465760545473316),\n",
       " (4, 0.09779651860947264),\n",
       " (5, 0.0768311020043291),\n",
       " (6, 0.044647079364953546),\n",
       " (7, 0.14869087541838666),\n",
       " (8, 0.0594149220504515),\n",
       " (9, 0.058693449766686084),\n",
       " (10, 0.0752940831920687),\n",
       " (11, 0.09269158016746293),\n",
       " (12, 0.12637604504456942),\n",
       " (13, 0.05548940236173458),\n",
       " (14, 0.048119773530762536),\n",
       " (15, 0.11563755034289788),\n",
       " (16, 0.15284337676394705),\n",
       " (17, 0.05204048052623202),\n",
       " (18, 0.08755548468874456),\n",
       " (19, 0.09358987959638854),\n",
       " (20, 0.07299980971594074),\n",
       " (21, 0.059542978082551205),\n",
       " (22, 0.09844101530493939),\n",
       " (23, 0.07107804724178875),\n",
       " (24, 0.05376809713533608),\n",
       " (25, 0.06124211612359264),\n",
       " (26, 0.03723792415725086),\n",
       " (27, 0.04855951421759549),\n",
       " (28, 0.0757269293184007),\n",
       " (29, 0.04535985563850467),\n",
       " (30, 0.06956530146846655),\n",
       " (31, 0.05710618834281626),\n",
       " (32, 0.0999121663575114),\n",
       " (33, 0.04148685322345651),\n",
       " (34, 0.04822475480586997),\n",
       " (35, 0.037850811440503555),\n",
       " (36, 0.053632424593772494),\n",
       " (37, 0.0860671705766327),\n",
       " (38, 0.0757269293184007),\n",
       " (39, 0.0516667905018637),\n",
       " (40, 0.07277125290702675),\n",
       " (41, 0.08589486954285151),\n",
       " (42, 0.08170755567029211),\n",
       " (43, 0.2022164238729813),\n",
       " (44, 0.038488413279857636),\n",
       " (45, 0.09963367635980339),\n",
       " (46, 0.11936721949303647),\n",
       " (47, 0.051444992454782304),\n",
       " (48, 0.04669652612595705),\n",
       " (49, 0.04372638370883651),\n",
       " (50, 0.10375734873558932),\n",
       " (51, 0.08857111762818307),\n",
       " (52, 0.09090578170276498),\n",
       " (53, 0.07594732772820081),\n",
       " (54, 0.04988942209275058),\n",
       " (55, 0.04118906025707944),\n",
       " (56, 0.04158479966752268),\n",
       " (57, 0.06640591021716188),\n",
       " (58, 0.17178973908570302),\n",
       " (59, 0.030661561120965963),\n",
       " (60, 0.06951356188864988),\n",
       " (61, 0.029945344108724296),\n",
       " (62, 0.036240922620528734),\n",
       " (63, 0.025906766249052145),\n",
       " (64, 0.045809787387367476),\n",
       " (65, 0.05292109368127936),\n",
       " (66, 0.052907423783328716),\n",
       " (67, 0.09509904687681307),\n",
       " (68, 0.0927414298719151),\n",
       " (69, 0.12994524234819715),\n",
       " (70, 0.07361669107049022),\n",
       " (71, 0.05054223880868187),\n",
       " (72, 0.12481312536979634),\n",
       " (73, 0.07077991184914895),\n",
       " (74, 0.06658058496522756),\n",
       " (75, 0.12557759159280324),\n",
       " (76, 0.12188080730883868),\n",
       " (77, 0.11392556553842695),\n",
       " (78, 0.07828611182447046),\n",
       " (79, 0.08137117420976155),\n",
       " (80, 0.051083366144114836),\n",
       " (81, 0.08113940009024605),\n",
       " (82, 0.04171256742022147),\n",
       " (83, 0.06158217488848265),\n",
       " (84, 0.07242365166045298),\n",
       " (85, 0.04673524830757169),\n",
       " (86, 0.09062467666925683),\n",
       " (87, 0.06869087964242851),\n",
       " (88, 0.09926276123075065),\n",
       " (89, 0.14762505217536087),\n",
       " (90, 0.047805813050885035),\n",
       " (91, 0.08187813857664124),\n",
       " (92, 0.05396992223664626),\n",
       " (93, 0.12303323029524592),\n",
       " (94, 0.08425708479079383),\n",
       " (95, 0.08232938778709988),\n",
       " (96, 0.13514578861415075),\n",
       " (97, 0.07355118704102556),\n",
       " (98, 0.08417898795020168),\n",
       " (99, 0.05028652300612805),\n",
       " (100, 0.060675234312012445),\n",
       " (101, 0.08986021044624994),\n",
       " (102, 0.13449471259176798),\n",
       " (103, 0.06279614726455619),\n",
       " (104, 0.20356780028723986),\n",
       " (105, 0.07032376125602535),\n",
       " (106, 0.3979994006871626),\n",
       " (107, 0.12764449509100936),\n",
       " (108, 0.07897265061488053),\n",
       " (109, 0.0245308469129331),\n",
       " (110, 0.05564787048299042),\n",
       " (111, 0.11127369155111752),\n",
       " (112, 0.06609001926936439),\n",
       " (113, 0.11882286737937243),\n",
       " (114, 0.09981885578382588)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 33s, sys: 4.2 s, total: 6min 37s\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dic[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dic.id2token\n",
    "\n",
    "model = LdaMulticore(\n",
    "    corpus=corpus_tfidf,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    workers=None,\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the topics and their probabilities for one document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.83751667), (16, 0.06718511)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.get_document_topics(corpus_tfidf[0], minimum_probability=None)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the words describing the topics for the same document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('charact', 0.0022977034), ('stori', 0.0022455796), ('like', 0.0021126294), ('time', 0.0019672627), ('good', 0.0019669973), ('love', 0.001953449), ('scene', 0.0019352934), ('great', 0.0019019066), ('watch', 0.0018573694), ('plai', 0.0017727332), ('peopl', 0.0017476061), ('look', 0.0017047834), ('end', 0.0016921982), ('wai', 0.0016395828), ('think', 0.0016117092), ('life', 0.0016099963), ('work', 0.0015607986), ('actor', 0.001550181), ('act', 0.0015447924), ('year', 0.0015330676)]\n",
      "[('jacki', 0.007437012), ('chan', 0.0073808166), ('kung', 0.005362623), ('stan', 0.0052415095), ('laurel', 0.005238125), ('kong', 0.004725162), ('hong', 0.004513635), ('hardi', 0.004380598), ('martial', 0.0039120535), ('wire', 0.0031765273), ('grier', 0.0030849183), ('olli', 0.002839588), ('homer', 0.0028322577), ('pam', 0.0028288807), ('monologu', 0.0027881463), ('greed', 0.0027833104), ('ebert', 0.0027241746), ('sinatra', 0.002718796), ('lamb', 0.0027081873), ('quinn', 0.0027063782)]\n"
     ]
    }
   ],
   "source": [
    "for i in x:\n",
    "    print(model.show_topic(i[0], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when using tfidf vector, the topics are almost always topic 8, which is related to scenes and watching and movies... makes sense since the documents are movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using corpus bow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 55s, sys: 12.8 s, total: 4min 8s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dic[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dic.id2token\n",
    "\n",
    "model = LdaMulticore(\n",
    "    corpus=corpus_bow,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    workers=None,\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to using only corpus bow, using the tfidf is achieved in a faster time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the topics with the topn words that describe them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 753 ms, sys: 22 ms, total: 775 ms\n",
      "Wall time: 774 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "top_topics = model.top_topics(corpppp, topn=5)\n",
    "#pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the top 100 topics, with the 5 words that, with highest probability, represent those topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the topics and their probabilities for one document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.13438915), (22, 0.12401116), (34, 0.2252998)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.get_document_topics(corpppp[0], minimum_probability=0.1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how document 0 (represented in terms of BOW as corpppp[0]) has its topics, with the probability of each topic. In this case, the document corresponds to topic 13 with 0.164 probability, and topic 40 with 0.136 probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the words describing the topics for the same document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('school', 0.013332094), ('boi', 0.008467523), ('girl', 0.008024706), ('like', 0.006921068), ('children', 0.0065888925), ('student', 0.00607882), ('high', 0.0058694812), ('kid', 0.0055167074), ('parent', 0.0051464), ('child', 0.004577135)]\n",
      "[('like', 0.013654727), ('holm', 0.0132337585), ('sex', 0.013080628), ('porn', 0.010436354), ('look', 0.008345899), ('scene', 0.007331529), ('shirlei', 0.0050096326), ('bone', 0.004902359), ('ellen', 0.004652755), ('jennif', 0.004401875)]\n",
      "[('horror', 0.023603853), ('bad', 0.02096437), ('like', 0.016407525), ('look', 0.01231438), ('good', 0.012066251), ('effect', 0.011218209), ('act', 0.010566096), ('watch', 0.009061358), ('scene', 0.0086893225), ('budget', 0.007922259)]\n"
     ]
    }
   ],
   "source": [
    "for i in x:\n",
    "    print(model.show_topic(i[0], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows for document 0, the top 10 words relating to its topic. Recall we had two topics for document 0, so the first list is the top 10 words that explain the first topic, and the second list is for the second topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF VS CORPUS BOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'P\\' (or Club-P) should really be called \\'L\\' for lame. Every festival has a disappointment and this is the one that fails to live up to its much-hyped logline: \"Thai lesbians fighting monsters.\" Rather, this is the tale of a Khmer country girl who\\'s grandmother has taught her a little witchcraft along with a few odd (but specific) rules: \"don\\'t walk under a clothesline,\" \"don\\'t eat raw meat,\" and \"don\\'t accept money for your powers.\" Well, guess what folks, the girl moves to Bangkok to raise some money as a \\'bar-girl\\' and manages to break all the rules granny taught her which subsequently releases an evil spirit that conveniently kills the \\'foreign johns\\' who pay for her services.<br /><br />While this film can\\'t even be released in Thailand due to it\\'s controversial subject matter most American audiences will find this ho-hum horror pic a cross between \"Showgirls\" and \"Interview with the Vampire\" as directed by Walt Disney.<br /><br />If not for a few scenes with significant amounts of blood the MPAA could probably rate this for pre-teens only. There is literally broadcast TV adult fare, although you\\'d expect at least a sex scene considering the fact that the film is about a brothel and one of the actresses is a Thai porn star in real life.<br /><br />As for the \\'lesbian\\' angle, there\\'s one brief smooch and a couple of \"I love you\\'s\" to prove that the two main stars really are a couple (on brother). And the P-bar has got to be the only exotic dance club on the planet where the girls keep their sarongs on and do carnival stunts (there\\'s a swordsman who cuts cucumbers out of a girl\\'s mouth ... ooh, phallic imagery).<br /><br />NO nudity, NO real monster (unless you count a five foot high Thai spirit with yellow eyes), and no way any ADULT should ever pay to see this kind of stupidity except on DVD. You\\'ve been warned!'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tfidf vs normal bow has some differences. Using tfidf runs in 6min30seconds compared to 4minutes14seconds for corpus bow.\n",
    "\n",
    "When checking for the first document, the normal bow makes more sense in terms of topics relating to each document, compared to tfidf which identifies most documents as having the same topic; movies and series... This is true, but we are more interested in the individual topics, and tfidf does a good job to giving importance to the words that appear in most documents, in this case movies appears in most documents, but this does not help us identify the sub topic for each document; porn in movies vs children/highschool... in movies. Using normal bow is better at this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
